{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb8df74",
   "metadata": {},
   "source": [
    "# Об эффекте дополнительного горизонтирования коэффициента линейной связи двух случайных величин \"шумами\" данных, или почему все-таки \"регрессия\"\n",
    "\n",
    "## Параметры публикации\n",
    "\n",
    "### Тип публикации / формат:  Статья / Кейс\n",
    "(максимально ***короткая*** статья)\n",
    "\n",
    "### Уровень сложности: Средний\n",
    "(ближе к простому, но все же требует минимума знаний по ключевым словам ниже)\n",
    "\n",
    "### Целевая аудитория: \n",
    "* начинающие DA/DS \n",
    "* или опытные пере- и само-учки со случайными белыми пятнами в образовании по DA/DS\n",
    "\n",
    "### Ключевые слова:\n",
    "* Простая линейная регрессия\n",
    "* Introduction to Statistical Learning\n",
    "* Python, Pandas, Scikit Learn\n",
    "\n",
    "## Введение\n",
    "\n",
    "Как сказал Йозеф Швейк, войдя в одно *очень уважаемое* заведение, \"Добрый вечер всей честной компании\" - от себя мне осталось лишь присовокупить к этой блестящей фразе \"пользователей контента Хабра!\"  Прошу, однако же, в отличие от истории Швейка, не встречать мое приветствие \"тычками под ребра\" и комментариями про идиотизм автора, решившегося представить свой первый опус взыскательной публике.\n",
    "\n",
    "Речь пойдет о небольшом поучительном факте в области линейной регрессии, который будет показан экспериментально и объяснен аналитическими формулами на примере простой линейной регрессии.  Этот факт, наверняка, известен тем, кто основательно изучал тему линейной регрессии регулярным образом, однако самоучки и выпускники интернет курсов, имя которыс сегодня легион благодаря популярности темы, вполне могли проскочить мимо него. По своему опыту, считаю, что знание данного факта будет практически полезно для коррекции их интуитивных представлений о линейной регрессии - что и сподвигло меня на написание данной заметки после того, как я сам его обнаружил, выполняя упражнения к книге \"An Introduction To Statistical Learning v2\" авторов Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.\n",
    "\n",
    "По-видимому, именно данный эффект исторически и привел к использованию термина \"регрессия\" - как известно, автор термина Фрэнсис Гальтон исследовал статистику роста в поколениях сыновей и отцов, и обнаружил, что линия определенно имеет наклон менее 45 градусов, из чего был сделан известный вывод о вырождении индивидуальности роста человека в будущих поколениях к некому среднему значению роста в его работе \"Регрессия к посредственности при наследовании роста\".\n",
    "\n",
    "Читатель, которому более близко познание мира через свой опыт, может прекратить дальнейшее чтение и самостоятельно обнаружить все написанное дельше, проделав упражнения номер 11 и 12 к главе №3 \"Linear Regression\" книги \"An Introduction To Statistical Learning v2\" авторов Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, доступной для скачивания на сайте авторов. \n",
    "\n",
    "## Кратко о сути эффекта\n",
    "\n",
    "### Психологическая преамбула\n",
    "\n",
    "В простой линейной регрессии принято полагать связь между истинными случайными величинами X и Y описываемой формулой \n",
    "$$ Y\\ =\\ \\beta_0 + \\beta_1\\cdot X + \\varepsilon ,$$ \n",
    "где матожидание случайной величины $ E(\\varepsilon)=0 $.  \n",
    "И даже том случае, если истинная связь нелинейна, мы предполагаем возможность ее аппроксимации в каком-то диапазоне Х такой формулой, убирая нелинейные эффекты в $\\varepsilon$.\n",
    "\n",
    "Имея конкретную выборку объектов $\\{x_i,y_i\\} \\,\\mbox{, где}\\, i \\in \\{1,...n\\}$, мы строим модель простой линейной регрессии (далее SLR) по формуле\n",
    "$$ y_i\\, =\\, \\hat\\beta_0 + \\hat\\beta_1\\cdot x_i + \\hat\\epsilon_i $$\n",
    "\n",
    "Если выборка является примером из реальной жизни, то мы не знаем истинных $\\beta_0$ и $\\beta_1$ и \"горды сознанием\", взирая на результаты работы своей модели SLR: $\\hat\\beta_0$ и $\\hat\\beta_1$, обозначенные знаком крышечки сверху.  Более того, если мы самостоятельно генерим выборку для численного эксперимента, используя рандомные функции по той же логике:\n",
    "* сперва генерим выборку на основе какого-то распределения для $x_i$,\n",
    "* выбираем для сокращения дальнейших рассуждений, но без снижения общности выводов: $\\beta_1 > 0$,\n",
    "* потом берем линейный член по $y_i$ и добавляем, например, стардарное нормальное распределение для члена с $\\epsilon$:  $y_i\\, =\\, \\beta_0 + \\beta_1\\cdot x_i + N(0,1)_i  $,\n",
    "\n",
    "то тренируя модель SLR на сгенеренной выборке (у которой мы уже знаем истинные коэффициенты $\\beta_0$ и $\\beta_1$!) мы получим также, что ни при какой большой статистической мощности выборки нельзя отвергнуть нулевую гипотезу $H_0:\\, \\hat\\beta_1 = \\beta_1$, что наполняет нас радостью от того, что статистическая наука не подкачала, ну и мы нигде не \"налажали\"!\n",
    "\n",
    "Однако попытка интуитивно ответить на простой вопрос \"каково будет ожидаемое значение коеффициента наклона регрессионной прямой при обратной линейной регрессии X на Y **на той же самой выборке** (то есть никакой новой генерации - только столбцы X и Y переименовали) необратимо поделит незнакомых с данным эффектом начинающих DA/DS-ов на два непересекающихся в одном измерении кластера:\n",
    "* на успешных в будущем бизнес консультантов, которые не моргнув глазом подгонят любые результаты под интуитивные желания бизнеса, за что будут обласканы момоной,\n",
    "* и на честных ботаников, которые, нарушив требования гигиены труда к разумному чередованию работы и отдыха и разочаровав начальство переносами дедлайна, все же придут к неумолимому выводу, что для обратной регрессии наклон кривой будет *статистически значимо* отличаться от интуитивно ожидаемого значения $ 1\\,/\\,\\beta_1 $.\n",
    "\n",
    "Введя обозначения для коэффициентов обратной регрессии как\n",
    "$$ x_i = \\hat\\beta_0^T + \\hat\\beta_1^T\\cdot y_i + \\hat\\epsilon_i^T ,$$\n",
    "мы можем переписать неумолимый вывод честных ботаников как тот факт, что при достаточно большой выборке всегда можно статистически значимо утверждать, что верна альтернативная гипотеза $H_1:\\, \\hat\\beta_1^T \\ne 1/\\beta_1$ - здесь индекс $T$ означает смену местами X и Y (аналог транспонирования).\n",
    "\n",
    "Когнитивный диссонанс обманутой интуиции побуждает нас, честных ботаников, разобраться в чем причина.\n",
    "\n",
    "### Формулы\n",
    "По счастью, в простой линейной регрессии аналитические формулы для коэффициентов получаются без матричных выражений, и даже (способный к математике) старшеклассник сможет их преобразовывать.  Поэтому, не оскорбляя читателя сомнениями в его способностях, сразу приведем конечные выражения для связи $\\hat\\beta_1$ и $\\hat\\beta_1^T$ в симетричном относительно обмена X и Y виде:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{eqnarray}\n",
    "&\\hat\\beta_1\\cdot\\hat\\beta_1^T &=& \\hat R^2 \\\\\n",
    "&\\hat\\beta_1\\cdot D(x_i) &=&\n",
    "\\hat\\beta_1^T\\cdot D(y_i) \\\\\n",
    "\\end{eqnarray}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "где D - это квадратичная дисперсия соответствующей величины на выборке, а значение характерной метрики $\\hat R^2$ равно для двумерного случая квадрату коррелятора $<X,Y>^2$ на данной выборке.  **NB!:** Причем, все это не асимптотические формулы при размере выборки, стремящемся к бесконечности, но абсолютно точные формулы, строго выполняющиеся для произвольной выборки любого размера.\n",
    "\n",
    "Полные выкладки по получению данных формул из условия минимума квадратичной функции потерь можно найти, например, вот здесь:  https://github.com/SanSanychSeva/Exercises-from-Introduction-to-Statistical-Learning-done-in-Python/blob/main/chapter_03_Linear_Regression/SLR%20formulas%20from%20ISLRv2.ipynb\n",
    "\n",
    "#### Некоторые быстрые выводы из симметричности системы уравнений для коэффициентов наклонов кривых\n",
    "* Как видим, только при полной корреляции X и Y верно интуитивное ожидание, что $\\hat\\beta_1^T = 1/\\hat\\beta_1$, иначе же имеем заниженное значение (при положительном $\\beta_1$ - в общем случае следует говорить про \"более горизонтальную\" прямую регрессии).  \n",
    "* Который именно из коэффициентов будет *сильнее* занижен относительно \"истинных значений\" $\\beta_0$ и $\\beta_1$ - это определяется балансом дисперсий обеих величин.  Может быть занижет только один из них - см. ниже в каком случае.\n",
    "* В частности, если дисперсии X и Y одинаковы, то $\\hat\\beta_1 = \\hat\\beta_1^T = <X,Y>$, а так как коррелятор меньше 1, то имеем очевидное объяснение эффекта регрессии взрослого роста при сравнении статистик в разных поколениях.  Забавно, но если бы Фрэнсис Гальтон повторил свое исследование, поменяв или случайно перепутав в данных роста где у него отцы, а где дети, то эффект был бы одинаков в обе стороны по времени (при условии, что статистическая погрешность измерения роста в разные эпохи была одинакова).\n",
    "\n",
    "### Интерпретация и филосовские выводы\n",
    "\n",
    "При генерации выборки объектов мы на самом деле заложили в логику генерации отсутствие стохастичности X - именно поэтому регулярная часть Y описывалась как $ Y_{рег} = \\beta_0 + \\beta_1\\cdot X$.  То есть мы рассматривали X как некоторую совокупность точных значений.  Строго говоря этого никогда не бывает - в данных есть стохастические шумы, и нам следовало бы генерить данные по принципу:\n",
    "* $X = X_{рег} + St_x$\n",
    "* $Y = \\beta_0 + \\beta_1\\cdot X_{рег} + St_y + \\varepsilon_y $\n",
    "\n",
    "где \"Штос-член\" определял бы стохастичность измерения истиного значения каждой переменной, а $\\varepsilon_y$ аггрегировал бы в себе усреднение зависимости истинных переменных по популяционному распределению неучитываемых других предикторов при регрессии Y на X.  В этом случае уже при прямой регрессии Y на X у нас было бы видно, что статистически верна альтернативная гипотеза $H_1:\\, \\hat\\beta_1 \\ne \\beta_1$, что собственно и наблюдал автор термина \"регрессия\" в своем исследовании.\n",
    "\n",
    "#### Объяснение эффекта \"на пальцах\"\n",
    "Предположим, что никакой стохастизации нет, коррелятор <X,Y> = 1, то есть все точки выборки лежат на \"истинной\" прямой. \n",
    "Возьмем теперь пару объектов выборки, которые лежат в небольшой окресности точки $(x_0,y_0)$ - для конкретности, правее \"центра масс\" выборки $(\\bar x,\\bar y)$. Пока данные объекты лежат на \"истинной прямой\", оба объекта \"голосуют\" за истинный наклон.  Теперь смоделируем влияние стохастизации по разным осям на регрессию Y по X:\n",
    "1. Если объекты раздвинуть по оси X в разные стороны на расстояние $\\delta$ так, что теперь они \"тянут\" регрессионную прямую в разные стороны, то у них появится вращающий момент во влиянии на наклон регрессионной прямой (Y на X), приближающий ее в горизонтальному положению - так как у них теперь разные плечи по оси X: $ x_0-\\bar x - \\delta$ и $ x_0-\\bar x + \\delta$.\n",
    "2. Если объекты аналогично раздвинуть по оси Y, дополнительного вращающего момента во влиянии на наклон регрессионной (Y на X) прямой не возникнет - так как у них обоих плечи по оси X не поменялись, оставшись равными $ x_0-\\bar x $.\n",
    "\n",
    "Именно поэтому в стандартном численном эксперименте выше получаем невозможность опревергнуть нулевую гипотезу $H_0:\\, \\hat\\beta_1 = \\beta_1$: стохастизация только Y при генерации датасета не горизонтирует регрессионную прямую для Y на X.  А вот когда мы меняем местами X и Y, то в обратной регрессии горизонтальной осью становится бывший Y, в результате регрессионная прямая становится более горизонтальной, чем $1/\\beta_1$\n",
    "\n",
    "**Замечание**: кажется странным, что при регрессии Y на X речь идет только о плече вдоль оси X - поскольку интуитивно мы понимаем регрессионную прямую как рычаг с точкой опоры в \"центре масс\" $(\\bar x,\\bar y)$, и рычагом должна служить не проекция на ось X, а проекция на сам рычаг.  Но причина в том, что в линейной регрессии функцией потерь является не сумма квадратов расстояний до регрессионной прямой в пространстве $X\\bigotimes Y$, но сумма квадратов residual errors $y_i-\\hat y_i$, где крышечка над $y_i$ означает предсказанное на объекте $x_i$ значение: $\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1\\cdot x_i$.\n",
    "\n",
    "#### Выводы\n",
    "1. Но ведь у нас любые физические величины имеют погрешность измерения, а реальные данные - ненулевой уровень стохастического шума.  Тогда все вышесказанное означает, что при линейной регрессии мы в принципе получаем всегда только оценку снизу для степени \"истинной\" зависимости Y(X), отраженной в любом эксперименте.  Чем больше шума в предикторе, тем меньше физического смысла в результате линейной регрессии.  Оценку сверху мы получаем проделав обратную регрессию.  Таким образом при обработке данных любого эксперимента мы получаем диапазон, внутри которого лежит \"истинная\" взаимосвязь двух физических величин.\n",
    "2. Оставив в стороне Штос-член, рассмотрим стохастический член $\\varepsilon$.  Как уже говорилось, по факту, в нем аггрегировано влияние других предикторов при их свертке до двумерной зависимости между Y и X. При этом для регрессии Y на X мы проводим данную свертку при постоянных X, а для регрессии Х на Y мы проводим данную свертку при постоянных Y - получая в общем случае разные $\\varepsilon_y$ и $\\varepsilon_х$. Но тогда для выбора более точной модели моджет оказаться принципиально заранее знать, что же все-таки причинно-следственно от чего зависит в реальной жизни - Y от X или X от Y. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90c526",
   "metadata": {},
   "source": [
    "## Численный эксперимент\n",
    "Проведем эксперимент для простой иллюстрации теории выше - не будем загромождать статью строгими проверками гипотез вида $H_0:\\, \\hat\\beta_1 = \\beta_1$.  Если кто-то желает увидеть более строгий анализ с t-статистикой для  $\\hat\\beta_k$ - можно посмотреть, например, здесь: https://github.com/SanSanychSeva/Exercises-from-Introduction-to-Statistical-Learning-done-in-Python/blob/main/chapter_03_Linear_Regression/exercise_11and12.ipynb).\n",
    "\n",
    "Воспользуемся библиотеками Питон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bfabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import scipy.stats as statmath \n",
    "from sklearn import linear_model\n",
    "\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a28407",
   "metadata": {},
   "source": [
    "### Генерация первой выборки: стохастизация только по Y, разные дисперсии X и Y\n",
    "* возьмем $\\beta_0 = 0$, a $\\beta_1 = 2$,\n",
    "* тогда $\\beta_1^T = 1/2$,\n",
    "* так как мы не хотим загромождать статью деталями оценки p-value определения самих $\\hat\\beta_k$, просто возьмем размер выборки побольше - чтобы произвольность генерации данных не вносила отклонений в сравнения $\\hat\\beta_k$ с $\\beta_k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c503a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta_1 = 2\n",
    "std_ey = 1\n",
    "n_sampling = 1_000_000\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "X1 = rng.standard_normal(n_sampling)\n",
    "Y1 = true_beta_1*X1 + rng.normal(scale=std_ey, size=n_sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d7268",
   "metadata": {},
   "source": [
    "#### Прямая и обратная регрессии на первой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134d0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLR(X,Y):                                      # вынесем в функцию, для переиспользования на второй выборке\n",
    "    SLR_YontoX = linear_model.LinearRegression()\n",
    "    SLR_YontoX.fit(X.reshape(-1, 1),Y)\n",
    "\n",
    "    SLR_XontoY = linear_model.LinearRegression()\n",
    "    SLR_XontoY.fit(Y.reshape(-1, 1),X)\n",
    "\n",
    "    beta = SLR_YontoX.coef_[0]\n",
    "    betaT = SLR_XontoY.coef_[0]\n",
    "    CorrXY = np.corrcoef(X,Y)[0][1]\n",
    "    R2 = CorrXY**2\n",
    "    Dx = np.std(X)**2\n",
    "    Dy = np.std(Y)**2\n",
    "    \n",
    "    return beta, betaT, R2, Dx, Dy\n",
    "\n",
    "beta_1, betaT_1, R2_1, Dx_1, Dy_1 = SLR(X1,Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d167a8",
   "metadata": {},
   "source": [
    "#### Сравнение результатов эксперимента с теорией\n",
    "\n",
    "##### сравним регрессионные наклоны с \"истинными\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3d4bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"true\" beta_1</th>\n",
       "      <th>SLR beta_1</th>\n",
       "      <th>вывод</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YontoX</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.998032</td>\n",
       "      <td>близко к истинному наклону</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XontoY</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.400042</td>\n",
       "      <td>наклон явно менее выражен</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        \"true\" beta_1  SLR beta_1                       вывод\n",
       "YontoX            2.0    1.998032  близко к истинному наклону\n",
       "XontoY            0.5    0.400042   наклон явно менее выражен"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_df = pd.DataFrame(index=['YontoX','XontoY'])\n",
    "show_df['\"true\" beta_1'] = [true_beta_1, 1/true_beta_1]\n",
    "show_df['SLR beta_1']  = [beta_1, betaT_1]\n",
    "show_df['вывод'] = ['близко к истинному наклону', 'наклон явно менее выражен']\n",
    "show_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181325c3",
   "metadata": {},
   "source": [
    "##### сравним левую и правую части равенств в симментричной системе для $\\hat\\beta_1$ и $\\hat\\beta_1^T$\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{eqnarray}\n",
    "&\\hat\\beta_1\\cdot\\hat\\beta_1^T &=& \\hat R^2 \\\\\n",
    "&\\hat\\beta_1\\cdot D(x_i) &=&\n",
    "\\hat\\beta_1^T\\cdot D(y_i) \\\\\n",
    "\\end{eqnarray}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a27a73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>левая часть уравнения</th>\n",
       "      <th>правая часть уравнения</th>\n",
       "      <th>вывод</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>первое уравнение системы</th>\n",
       "      <td>0.799297</td>\n",
       "      <td>0.799297</td>\n",
       "      <td>полное совпадение</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>второе уравнение системы</th>\n",
       "      <td>1.997861</td>\n",
       "      <td>1.997861</td>\n",
       "      <td>полное совпадение</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         левая часть уравнения правая часть уравнения  \\\n",
       "первое уравнение системы              0.799297               0.799297   \n",
       "второе уравнение системы              1.997861               1.997861   \n",
       "\n",
       "                                      вывод  \n",
       "первое уравнение системы  полное совпадение  \n",
       "второе уравнение системы  полное совпадение  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_df = pd.DataFrame(index=['левая часть уравнения', 'правая часть уравнения', 'вывод'])\n",
    "show_df['первое уравнение системы']= [beta_1*betaT_1, R2_1, 'полное совпадение']\n",
    "show_df['второе уравнение системы'] = [beta_1*Dx_1, betaT_1*Dy_1, 'полное совпадение']\n",
    "show_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc7c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f65fce36",
   "metadata": {},
   "source": [
    "### Генерация второй выборки: стохастизация только по X, одинаковые дисперсии X и Y\n",
    "* оставим $\\beta_0 = 0$, $\\beta_1 = 2$ и прежний размер выборки,\n",
    "* по прежнему $\\beta_1^T = 1/2$,\n",
    "* обеспечим одинаковую асимптотическую дисперсию в исходных формулах генерации выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1f13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_ex = math.sqrt(true_beta_1**2 - 1)                 # это обеспечит одинаковую в асимптотике дисперсию по X и Y\n",
    "\n",
    "Z2 = rng.standard_normal(n_sampling)                   # это регулярная часть X\n",
    "Y2 = true_beta_1*Z2\n",
    "X2 = Z2 + rng.normal(scale=std_ex, size=n_sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49091756",
   "metadata": {},
   "source": [
    "#### Прямая и обратная регрессии на второй выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "821f8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_2, betaT_2, R2_2, Dx_2, Dy_2 = SLR(X2,Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13b3be",
   "metadata": {},
   "source": [
    "#### Сравнение результатов эксперимента с теорией\n",
    "\n",
    "##### сравним регрессионные наклоны с \"истинными\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585cf69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"true\" beta_1</th>\n",
       "      <th>SLR beta_1</th>\n",
       "      <th>вывод</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YontoX</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.498901</td>\n",
       "      <td>близко к обратному значению</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XontoY</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.499387</td>\n",
       "      <td>близко к истинному наклону</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        \"true\" beta_1  SLR beta_1                        вывод\n",
       "YontoX            2.0    0.498901  близко к обратному значению\n",
       "XontoY            0.5    0.499387   близко к истинному наклону"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_df = pd.DataFrame(index=['YontoX','XontoY'])\n",
    "show_df['\"true\" beta_1'] = [true_beta_1, 1/true_beta_1]\n",
    "show_df['SLR beta_1']  = [beta_2, betaT_2]\n",
    "show_df['вывод'] = ['близко к обратному значению','близко к истинному наклону']\n",
    "show_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df874b2",
   "metadata": {},
   "source": [
    "##### сравним левую и правую части равенств в симментричной системе для $\\hat\\beta_1$ и $\\hat\\beta_1^T$\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{eqnarray}\n",
    "&\\hat\\beta_1\\cdot\\hat\\beta_1^T &=& \\hat R^2 \\\\\n",
    "&\\hat\\beta_1\\cdot D(x_i) &=&\n",
    "\\hat\\beta_1^T\\cdot D(y_i) \\\\\n",
    "\\end{eqnarray}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160c5903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>левая часть уравнения</th>\n",
       "      <th>правая часть уравнения</th>\n",
       "      <th>вывод</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>первое уравнение системы</th>\n",
       "      <td>0.249145</td>\n",
       "      <td>0.249145</td>\n",
       "      <td>полное совпадение</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>второе уравнение системы</th>\n",
       "      <td>1.996394</td>\n",
       "      <td>1.996394</td>\n",
       "      <td>полное совпадение</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         левая часть уравнения правая часть уравнения  \\\n",
       "первое уравнение системы              0.249145               0.249145   \n",
       "второе уравнение системы              1.996394               1.996394   \n",
       "\n",
       "                                      вывод  \n",
       "первое уравнение системы  полное совпадение  \n",
       "второе уравнение системы  полное совпадение  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_df = pd.DataFrame(index=['левая часть уравнения', 'правая часть уравнения', 'вывод'])\n",
    "show_df['первое уравнение системы']= [beta_2*betaT_2, R2_2, 'полное совпадение']\n",
    "show_df['второе уравнение системы'] = [beta_2*Dx_2, betaT_2*Dy_2, 'полное совпадение']\n",
    "show_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e3516",
   "metadata": {},
   "source": [
    "## Заключение по численному эксперименту\n",
    "* Нам удалось в численном эксперименте проиллюстрировать правильность симметричной системы уравнений для прямой и обратной регрессий на одной выборке объектов.  \n",
    "* Нам удалось в численном эксперименте проиллюстрировать правильность \"пальцевого\" объяснения, почему на дополнительное горизонтирование регрессионного наклона влияет только шум по оси X.\n",
    "* При этом манипулируя дисперсией для рандомных членов для генерации X и Y можно добиться отдельного совпадения либо коэффициента прямой регрессии, либо обратной с интуитивно ожидаемым \"идеальным\" значением.\n",
    "* Также шумами можно добится совпадения прямого и обратного регрессионного наклона, даже если \"истинная\" зависимость отлична от 1.\n",
    "* Поскольку мы не фиксировали seed при случайной генерации выборки, то числа в таблицах будут немного меняться при повторных прогонах кода, но это не изменит выводы в таблице."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
